{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成データのPlot形式を合わせた\n",
    "import sys\n",
    "sys.path.insert(0, './Utilities/')\n",
    "\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from pyDOE import lhs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# CUDAのサポート\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "K = 0.47 #部品の熱伝導率。[W/mK]\n",
    "rho = 1573 #部品の密度。[kg/m^3]\n",
    "cp = 967 #部品の比熱。[J/kgK]\n",
    "alpha = K / (rho * cp)\n",
    "\n",
    "h1 = 100\n",
    "h2 = 50\n",
    "\n",
    "#TODO4：損失関数の重み付け\n",
    "lambda_u, lambda_f, lambda_BC1, lambda_BC2 = 1.0, 1.0, 1.0, 1.0 \n",
    "\n",
    "N_u = 100\n",
    "N_f = 1000\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "# 最優先TODO：初期条件/境界条件の解を計算\n",
    "x = np.linspace(0, 0.2, 100)[:, None]  # 空間グリッド\n",
    "t = np.linspace(0, 5000, 1000)[:, None]  # 時間グリッド\n",
    "dx = x[1] - x[0]  # 空間ステップ\n",
    "dt = t[1] - t[0]  # 時間ステップ\n",
    "\n",
    "Nx = len(x)\n",
    "Nt = len(t)\n",
    "u = np.zeros((Nt, Nx))\n",
    "\n",
    "for n in range(0, Nt - 1):\n",
    "    if t[n] <= 600:\n",
    "        T_ext1 = T_ext2 = 0 + (50 - 0) * t[n] / 600\n",
    "    else:\n",
    "        T_ext1 = T_ext2 = 50\n",
    "    for i in range(1, Nx - 1):\n",
    "        u[n + 1, i] = u[n, i] + alpha * dt * (u[n, i - 1] - 2 * u[n, i] + u[n, i + 1]) / dx**2\n",
    "\n",
    "    # 境界条件の適用\n",
    "    # 左端の対流境界条件\n",
    "    u[n + 1, 0] = (4 * u[n + 1, 1] - u[n + 1, 2] + 2 * dx * h1 / K * T_ext1) / (3 + 2 * dx * h1 / K)\n",
    "\n",
    "    # 右端の対流境界条件\n",
    "    u[n + 1, -1] = (4 * u[n + 1, -2] - u[n + 1, -3] + 2 * dx * h2 / K * T_ext2) / (3 + 2 * dx * h2 / K)\n",
    "\n",
    "# 温度分布の可視化\n",
    "fig = plt.figure(figsize=(12, 10))  # 図のサイズを指定\n",
    "\n",
    "# 予測温度分布\n",
    "ax1 = fig.add_subplot(211)\n",
    "h1 = ax1.imshow(u.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "divider1 = make_axes_locatable(ax1)\n",
    "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar1 = fig.colorbar(h1, cax=cax1)\n",
    "cbar1.ax.tick_params(labelsize=15)\n",
    "ax1.set_title('Exact Temperature Distribution', fontsize=15)\n",
    "ax1.set_xlabel('Time [s]', fontsize=12)\n",
    "ax1.set_ylabel('Position [m]', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = u.flatten()[:, None]\n",
    "\n",
    "# ドメインの境界\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "# 初期条件と境界条件のデータを準備\n",
    "xx1 = np.hstack((X[0:1, :].T, T[0:1, :].T))\n",
    "uu1 = u[0:1, :].T\n",
    "xx2 = np.hstack((X[:, 0:1], T[:, 0:1]))\n",
    "uu2 = u[:, 0:1]\n",
    "xx3 = np.hstack((X[:, -1:], T[:, -1:]))\n",
    "uu3 = u[:, -1:]\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "X_f_train = lb + (ub - lb) * lhs(2, N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "X_u_train = X_u_train[idx, :]\n",
    "u_train = u_train[idx, :]\n",
    "\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        self.depth = len(layers) - 1\n",
    "        self.activation = torch.nn.ELU\n",
    "\n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1):\n",
    "            layer_list.append(('layer_%d' % i, torch.nn.Linear(layers[i], layers[i + 1])))\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "\n",
    "        layer_list.append(('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1])))\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "\n",
    "class PhysicsInformedNN():\n",
    "    def __init__(self, X_u, u, X_f, layers, lb, ub, k, rho, cp, alpha, h1, h2):\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "\n",
    "        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.k = k\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters(), lr=1e-4)\n",
    "\n",
    "        self.lambda_u = torch.tensor(lambda_u, requires_grad=True, device=device)\n",
    "        self.lambda_f = torch.tensor(lambda_f, requires_grad=True, device=device)\n",
    "        self.lambda_BC1 = torch.tensor(lambda_BC1, requires_grad=True, device=device)\n",
    "        self.lambda_BC2 = torch.tensor(lambda_BC2, requires_grad=True, device=device)\n",
    "\n",
    "    def net_t_inf(self, t):\n",
    "        t_inf = torch.where(t <= 600, 50 * t / 600, torch.tensor(50.0).to(device))\n",
    "        return t_inf\n",
    "\n",
    "    def net_u(self, x, t):\n",
    "        u = self.dnn(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "\n",
    "    def net_f(self, x, t):\n",
    "        u = self.net_u(x, t)\n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t,\n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x,\n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x,\n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        f = self.alpha * u_xx - u_t\n",
    "        return f\n",
    "\n",
    "    def net_BC1(self, x, t):\n",
    "        t_inf = self.net_t_inf(t)\n",
    "        u = self.net_u(x, t)\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x,\n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        BC1 = - (t_inf - u) + (self.k / self.h1) * u_x\n",
    "        return BC1\n",
    "\n",
    "    def net_BC2(self, x, t):\n",
    "        t_inf = self.net_t_inf(t)\n",
    "        u = self.net_u(x, t)\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x,\n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        BC2 = t_inf - u - (self.k / self.h2) * u_x\n",
    "        return BC2\n",
    "\n",
    "    def loss_func(self):\n",
    "        u_pred = self.net_u(self.x_u, self.t_u)\n",
    "        f_pred = self.net_f(self.x_f, self.t_f)\n",
    "        x_BC1 = torch.full((self.t_f.shape[0], 1), self.lb[0], requires_grad=True).to(device)\n",
    "        t_BC1 = self.t_f\n",
    "        BC1_pred = self.net_BC1(x_BC1, t_BC1)\n",
    "\n",
    "        x_BC2 = torch.full((self.t_f.shape[0], 1), self.ub[0], requires_grad=True).to(device)\n",
    "        t_BC2 = self.t_f\n",
    "        BC2_pred = self.net_BC2(x_BC2, t_BC2)\n",
    "\n",
    "        loss_u = torch.mean((self.u - u_pred)**2)\n",
    "        loss_f = torch.mean(f_pred**2)\n",
    "        loss_BC1 = torch.mean(BC1_pred**2)\n",
    "        loss_BC2 = torch.mean(BC2_pred**2)\n",
    "\n",
    "        total_loss = (\n",
    "            self.lambda_u * loss_u +\n",
    "            self.lambda_f * loss_f +\n",
    "            self.lambda_BC1 * loss_BC1 +\n",
    "            self.lambda_BC2 * loss_BC2\n",
    "        )\n",
    "\n",
    "        return total_loss, loss_u, loss_f, loss_BC1, loss_BC2\n",
    "\n",
    "    def train(self, nIter):\n",
    "        for epoch in range(nIter):\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            total_loss, loss_u, loss_f, loss_BC1, loss_BC2 = self.loss_func()\n",
    "            total_loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Adaptive normalization\n",
    "                self.lambda_u *= torch.exp(0.1 * loss_u.detach() / total_loss.detach())\n",
    "                self.lambda_f *= torch.exp(0.1 * loss_f.detach() / total_loss.detach())\n",
    "                self.lambda_BC1 *= torch.exp(0.1 * loss_BC1.detach() / total_loss.detach())\n",
    "                self.lambda_BC2 *= torch.exp(0.1 * loss_BC2.detach() / total_loss.detach())\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                # print(f'Epoch {epoch}: Total Loss: {total_loss:.4e}, Loss u: {loss_u:.4e}, Loss f: {loss_f:.4e}, Loss BC1: {loss_BC1:.4e}, Loss BC2: {loss_BC2:.4e}')\n",
    "                #重みと損失の値を表示\n",
    "                print(f'Epoch {epoch}: Total Loss: {total_loss.item():.4e}, Loss u: {loss_u.item():.4e}, Loss f: {loss_f.item():.4e}, Loss BC1: {loss_BC1.item():.4e}, Loss BC2: {loss_BC2.item():.4e}')\n",
    "                print(f'lambda_u: {self.lambda_u.item()}, lambda_f: {self.lambda_f.item()}, lambda_BC1: {self.lambda_BC1.item()}, lambda_BC2: {self.lambda_BC2.item()}')\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        x_star = torch.tensor(X_star[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t_star = torch.tensor(X_star[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        u_star = self.net_u(x_star, t_star)\n",
    "        f_star = self.net_f(x_star, t_star)\n",
    "\n",
    "        return u_star.detach().cpu().numpy(), f_star.detach().cpu().numpy()\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub, alpha, rho, cp, K, h1, h2)\n",
    "\n",
    "# モデルのトレーニング\n",
    "model.train(1000)\n",
    "\n",
    "# 予測\n",
    "u_pred, f_pred = model.predict(X_star)\n",
    "\n",
    "#消さない\n",
    "# 誤差の計算\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "Error = np.abs(u - U_pred)\n",
    "\n",
    "# 可視化\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar = fig.colorbar(h, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=15) \n",
    "\n",
    "ax.plot(\n",
    "    X_u_train[:,1], \n",
    "    X_u_train[:,0], \n",
    "    'kx', label = 'Data (%d points)' % (u_train.shape[0]), \n",
    "    markersize = 4, \n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "# 誤差の計算\n",
    "Error = np.abs(u - U_pred)\n",
    "\n",
    "# 温度分布の可視化\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "\n",
    "# 予測温度分布\n",
    "ax1 = fig.add_subplot(211)\n",
    "h1 = ax1.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "divider1 = make_axes_locatable(ax1)\n",
    "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar1 = fig.colorbar(h1, cax=cax1)\n",
    "cbar1.ax.tick_params(labelsize=15)\n",
    "ax1.set_title('Predicted Temperature Distribution')\n",
    "ax1.set_xlabel('Time [s]')\n",
    "ax1.set_ylabel('Position [m]')\n",
    "\n",
    "# 残差の可視化\n",
    "ax2 = fig.add_subplot(212)\n",
    "h2 = ax2.imshow(Error.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "divider2 = make_axes_locatable(ax2)\n",
    "cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar2 = fig.colorbar(h2, cax=cax2)\n",
    "cbar2.ax.tick_params(labelsize=15)\n",
    "ax2.set_title('Absolute Error Distribution')\n",
    "ax2.set_xlabel('Time [s]')\n",
    "ax2.set_ylabel('Position [m]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数の修正\n",
    "import sys\n",
    "sys.path.insert(0, './Utilities/')\n",
    "\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from pyDOE import lhs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# CUDAのサポート\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "K = 0.47 #部品の熱伝導率。[W/mK]\n",
    "rho = 1573 #部品の密度。[kg/m^3]\n",
    "cp = 967 #部品の比熱。[J/kgK]\n",
    "alpha = K / (rho * cp)\n",
    "\n",
    "h1 = 100\n",
    "h2 = 50\n",
    "\n",
    "lambda_u, lambda_f, lambda_BC0, lambda_BC1, lambda_BC2 = 1.0, 1.0, 1.0, 1.0, 1.0 \n",
    "\n",
    "N_u = 100\n",
    "N_f = 1000\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "x = np.linspace(0, 0.2, 100)[:, None]  # 空間グリッド\n",
    "t = np.linspace(0, 5000, 1000)[:, None]  # 時間グリッド\n",
    "dx = x[1] - x[0]  # 空間ステップ\n",
    "dt = t[1] - t[0]  # 時間ステップ\n",
    "\n",
    "Nx = len(x)\n",
    "Nt = len(t)\n",
    "u = np.zeros((Nt, Nx))\n",
    "\n",
    "for n in range(0, Nt - 1):\n",
    "    if t[n] <= 600:\n",
    "        T_ext1 = T_ext2 = 0 + (50 - 0) * t[n] / 600\n",
    "    else:\n",
    "        T_ext1 = T_ext2 = 50\n",
    "    for i in range(1, Nx - 1):\n",
    "        u[n + 1, i] = u[n, i] + alpha * dt * (u[n, i - 1] - 2 * u[n, i] + u[n, i + 1]) / dx**2\n",
    "\n",
    "    # 境界条件の適用\n",
    "    # 左端の対流境界条件\n",
    "    u[n + 1, 0] = (4 * u[n + 1, 1] - u[n + 1, 2] + 2 * dx * h1 / K * T_ext1) / (3 + 2 * dx * h1 / K)\n",
    "\n",
    "    # 右端の対流境界条件\n",
    "    u[n + 1, -1] = (4 * u[n + 1, -2] - u[n + 1, -3] + 2 * dx * h2 / K * T_ext2) / (3 + 2 * dx * h2 / K)\n",
    "\n",
    "# 温度分布の可視化\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "# 正確な温度分布\n",
    "ax1 = fig.add_subplot(211)\n",
    "h_img1 = ax1.imshow(u.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "divider1 = make_axes_locatable(ax1)\n",
    "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar1 = fig.colorbar(h_img1, cax=cax1)\n",
    "cbar1.ax.tick_params(labelsize=15)\n",
    "ax1.set_title('Exact Temperature Distribution', fontsize=15)\n",
    "ax1.set_xlabel('Time [s]', fontsize=12)\n",
    "ax1.set_ylabel('Position [m]', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = u.flatten()[:, None]\n",
    "\n",
    "# ドメインの境界\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "# 初期条件と境界条件のデータを準備\n",
    "xx1 = np.hstack((X[0:1, :].T, T[0:1, :].T))\n",
    "uu1 = u[0:1, :].T\n",
    "xx2 = np.hstack((X[:, 0:1], T[:, 0:1]))\n",
    "uu2 = u[:, 0:1]\n",
    "xx3 = np.hstack((X[:, -1:], T[:, -1:]))\n",
    "uu3 = u[:, -1:]\n",
    "\n",
    "X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "X_f_train = lb + (ub - lb) * lhs(2, N_f)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train))\n",
    "u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "X_u_train = X_u_train[idx, :]\n",
    "u_train = u_train[idx, :]\n",
    "\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        self.depth = len(layers) - 1\n",
    "        self.activation = torch.nn.ELU\n",
    "\n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1):\n",
    "            layer_list.append(('layer_%d' % i, torch.nn.Linear(layers[i], layers[i + 1])))\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "\n",
    "        layer_list.append(('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1])))\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "class PhysicsInformedNN():\n",
    "    def __init__(self, X_u, u, X_f, layers, lb, ub, rho, cp, alpha, k, h1, h2, T0):\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "\n",
    "        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.k = k\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "        self.T0 = T0\n",
    "\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters(), lr=1e-4)\n",
    "\n",
    "        self.lambda_f = torch.tensor(lambda_f, requires_grad=True, device=device)\n",
    "        self.lambda_BC0 = torch.tensor(lambda_BC0, requires_grad=True, device=device)\n",
    "        self.lambda_BC1 = torch.tensor(lambda_BC1, requires_grad=True, device=device)\n",
    "        self.lambda_BC2 = torch.tensor(lambda_BC2, requires_grad=True, device=device)\n",
    "\n",
    "    def net_t_inf(self, t):\n",
    "        t_inf = torch.where(t <= 600, 50 * t / 600, torch.tensor(50.0).to(device))\n",
    "        return t_inf\n",
    "\n",
    "    def net_u(self, x, t):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(1)\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(1)\n",
    "        u = self.dnn(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "\n",
    "    def net_f(self, x, t):\n",
    "        u = self.net_u(x, t)\n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t,\n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x,\n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x,\n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "\n",
    "        f = self.alpha * u_xx - u_t\n",
    "        return f\n",
    "    #新しく追加\n",
    "    def net_BC0(self, x):\n",
    "        t = torch.zeros_like(x)\n",
    "        u = self.net_u(x, t)\n",
    "        BC0 = self.T0 - u\n",
    "        return BC0\n",
    "    def net_BC1(self, x, t):\n",
    "        t_inf = self.net_t_inf(t)\n",
    "        u = self.net_u(x, t)\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x,\n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        BC1 = - (t_inf - u) + (self.k / self.h1) * u_x\n",
    "        return BC1\n",
    "\n",
    "    def net_BC2(self, x, t):\n",
    "        t_inf = self.net_t_inf(t)\n",
    "        u = self.net_u(x, t)\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x,\n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        BC2 = t_inf - u - (self.k / self.h2) * u_x\n",
    "        return BC2\n",
    "\n",
    "    def loss_func(self):\n",
    "        f_pred = self.net_f(self.x_f, self.t_f)\n",
    "\n",
    "        #新しく追加\n",
    "        x_BC0 = self.x_f[self.t_f == 0]\n",
    "        BC0_pred = self.net_BC0(x_BC0)\n",
    "\n",
    "        x_BC1 = torch.full((self.t_f.shape[0], 1), self.lb[0], requires_grad=True).to(device)\n",
    "        t_BC1 = self.t_f\n",
    "        BC1_pred = self.net_BC1(x_BC1, t_BC1)\n",
    "\n",
    "        x_BC2 = torch.full((self.t_f.shape[0], 1), self.ub[0], requires_grad=True).to(device)\n",
    "        t_BC2 = self.t_f\n",
    "        BC2_pred = self.net_BC2(x_BC2, t_BC2)\n",
    "\n",
    "        loss_f = torch.mean(f_pred**2)\n",
    "        # 新しく追加\n",
    "        loss_BC0 = torch.mean(BC0_pred**2)\n",
    "        loss_BC1 = torch.mean(BC1_pred**2)\n",
    "        loss_BC2 = torch.mean(BC2_pred**2)\n",
    "\n",
    "        total_loss = (\n",
    "            self.lambda_f * loss_f +\n",
    "            self.lambda_BC0 * loss_BC0 +\n",
    "            self.lambda_BC1 * loss_BC1 +\n",
    "            self.lambda_BC2 * loss_BC2\n",
    "        )\n",
    "\n",
    "        return total_loss, loss_f, loss_BC0, loss_BC1, loss_BC2\n",
    "\n",
    "    def train(self, nIter):\n",
    "        for epoch in range(nIter):\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            total_loss, loss_f, loss_BC0, loss_BC1, loss_BC2 = self.loss_func()\n",
    "            total_loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Adaptive normalization\n",
    "                self.lambda_f *= torch.exp(0.1 * loss_f.detach() / total_loss.detach())\n",
    "                self.lambda_BC0 *= torch.exp(0.1 * loss_BC0.detach() / total_loss.detach())\n",
    "                self.lambda_BC1 *= torch.exp(0.1 * loss_BC1.detach() / total_loss.detach())\n",
    "                self.lambda_BC2 *= torch.exp(0.1 * loss_BC2.detach() / total_loss.detach())\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}: Total Loss: {total_loss.item():.4e}, Loss f: {loss_f.item():.4e}, Loss BC0: {loss_BC0.item():.4e}, Loss BC1: {loss_BC1.item():.4e}, Loss BC2: {loss_BC2.item():.4e}')\n",
    "                print(f'lambda_f: {self.lambda_f.item()}, lambda_BC0: {self.lambda_BC0.item()}, lambda_BC1: {self.lambda_BC1.item()}, lambda_BC2: {self.lambda_BC2.item()}')\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        x_star = torch.tensor(X_star[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t_star = torch.tensor(X_star[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        u_star = self.net_u(x_star, t_star)\n",
    "        f_star = self.net_f(x_star, t_star)\n",
    "\n",
    "        return u_star.detach().cpu().numpy(), f_star.detach().cpu().numpy()\n",
    "\n",
    "T0 = u_train[X_u_train[:,1] == 0].mean().item()\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub, alpha, rho, cp, K, h1, h2, T0)\n",
    "\n",
    "# モデルのトレーニング\n",
    "model.train(5000)\n",
    "\n",
    "# 予測\n",
    "u_pred, f_pred = model.predict(X_star)\n",
    "\n",
    "# 誤差の計算\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))                     \n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "Error = np.abs(u - U_pred)\n",
    "\n",
    "\n",
    "# 以下のプロット部分も同様に変更します\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h_img2 = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar = fig.colorbar(h_img2, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=15) \n",
    "\n",
    "ax.plot(\n",
    "    X_u_train[:,1], \n",
    "    X_u_train[:,0], \n",
    "    'kx', label = 'Data (%d points)' % (u_train.shape[0]), \n",
    "    markersize = 4, \n",
    "    clip_on = False,\n",
    "    alpha=1.0\n",
    ")\n",
    "\n",
    "# 誤差の計算\n",
    "Error = np.abs(u - U_pred)\n",
    "\n",
    "# 温度分布の可視化\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "\n",
    "# 予測温度分布\n",
    "ax1 = fig.add_subplot(211)\n",
    "h_img3 = ax1.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "divider1 = make_axes_locatable(ax1)\n",
    "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar1 = fig.colorbar(h_img3, cax=cax1)\n",
    "cbar1.ax.tick_params(labelsize=15)\n",
    "ax1.set_title('Predicted Temperature Distribution')\n",
    "ax1.set_xlabel('Time [s]')\n",
    "ax1.set_ylabel('Position [m]')\n",
    "\n",
    "# 残差の可視化\n",
    "ax2 = fig.add_subplot(212)\n",
    "h_img4 = ax2.imshow(Error.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "divider2 = make_axes_locatable(ax2)\n",
    "cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar2 = fig.colorbar(h_img4, cax=cax2)\n",
    "cbar2.ax.tick_params(labelsize=15)\n",
    "ax2.set_title('Absolute Error Distribution')\n",
    "ax2.set_xlabel('Time [s]')\n",
    "ax2.set_ylabel('Position [m]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
