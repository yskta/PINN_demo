{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#アンダーダンピングぐされた調和振動し(減衰振動子)の解析解を計算\n",
    "def exact_solution(d, w0, t):\n",
    "    #d：減衰係数, w0:固有振動数, t:時間\n",
    "    assert d < w0\n",
    "    w = np.sqrt(w0**2-d**2)\n",
    "    phi = np.arctan(-d/w)\n",
    "    A = 1/(2*np.cos(phi))\n",
    "    cos = torch.cos(phi+w*t)\n",
    "    exp = torch.exp(-d*t)\n",
    "    u = exp*2*A*cos\n",
    "    return u\n",
    "\n",
    "#PyTorchを用いて全結合ニューラルネットワークを定義\n",
    "#FCNクラスはnn.Moduleクラスを継承\n",
    "class FCN(nn.Module):\n",
    "    #ネットワークの構造を初期化\n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        #N_INPUT:入力次元数, N_OUTPUT:出力次元数, N_HIDDEN:隠れ層の次元数, N_LAYERS:隠れ層の数\n",
    "        super().__init__()\n",
    "        #活性化関数はTanh関数\n",
    "        activation = nn.Tanh\n",
    "        # 最初の全結合層と活性化関数\n",
    "        self.fcs = nn.Sequential(*[\n",
    "                        nn.Linear(N_INPUT, N_HIDDEN),\n",
    "                        activation()])\n",
    "        # 隠れ層の連続を定義\n",
    "        #ここでは、隠れ層をN_LAYERS-1個作成\n",
    "        #それぞれの隠れ層は、N_HIDDEN次元の入力を受け取り、N_HIDDEN次元の出力に変換し、その後Tanh関数を適用\n",
    "        #nn.Sequentialをネストして使い、各隠れ層とその活性化関数を順に適用\n",
    "        self.fch = nn.Sequential(*[\n",
    "                        nn.Sequential(*[\n",
    "                            nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "                            activation()]) for _ in range(N_LAYERS-1)])\n",
    "        # 最後の全結合層\n",
    "        #この層は、隠れ層からの出力を、N_OUTPUT次元に変換する\n",
    "        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)\n",
    "    #順伝播計算を定義\n",
    "    #入力xに対して、最初にfcsを適用し、その後fchを適用し、最後にfceを適用\n",
    "    #最終的な出力を返す\n",
    "    def forward(self, x):\n",
    "        x = self.fcs(x)\n",
    "        x = self.fch(x)\n",
    "        x = self.fce(x)\n",
    "        return x\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "\n",
    "#ニューラルネットワークのインスタンスを作成\n",
    "#これ自体はPINNではなく、PINNの一部として使われる\n",
    "pinn = FCN(1,1,32,3)\n",
    "\n",
    "# 境界条件と物理条件の訓練データを定義\n",
    "# TODO # (1, 1)\n",
    "t_boundary = torch.tensor(0.).view(-1,1).requires_grad_(True)# (1, 1)\n",
    "\n",
    "# define training points over the entire domain, for the physics loss\n",
    "# TODO # (30, 1)\n",
    "t_physics = torch.linspace(0,1,30).view(-1,1).requires_grad_(True)# (30, 1)\n",
    "\n",
    "# train the PINN\n",
    "d, w0 = 2, 20\n",
    "mu, k = 2*d, w0**2\n",
    "t_test = torch.linspace(0,1,300).view(-1,1)\n",
    "u_exact = exact_solution(d, w0, t_test)\n",
    "optimiser = torch.optim.Adam(pinn.parameters(),lr=1e-3)\n",
    "for i in range(15001):\n",
    "    #勾配を0に初期化\n",
    "    #PyTorchでは、勾配（gradients）は逆伝播（backpropagation）によって計算されますが、この計算は累積されます。\n",
    "    #具体的には、loss.backward()を呼び出すたびに、計算された勾配が既存の勾配に加算されます。\n",
    "    #これは、多くのトレーニングループで、誤った勾配の累積が発生する可能性があるため、次のトレーニングステップを開始する前に勾配をリセットする必要あり\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    # compute each term of the PINN loss function above\n",
    "    # using the following hyperparameters\n",
    "    lambda1, lambda2 = 1e-1, 1e-4\n",
    "    \n",
    "    #境界条件の損失を計算\n",
    "    # TODO\n",
    "    u = pinn(t_boundary)# (1, 1)\n",
    "    loss1 = (torch.squeeze(u) - 1)**2\n",
    "    dudt = torch.autograd.grad(u, t_boundary, torch.ones_like(u), create_graph=True)[0]# (1, 1)\n",
    "    loss2 = (torch.squeeze(dudt) - 0)**2\n",
    "    \n",
    "    #物理条件の損失を計算\n",
    "    # TODO\n",
    "    u = pinn(t_physics)# (30, 1)\n",
    "    dudt = torch.autograd.grad(u, t_physics, torch.ones_like(u), create_graph=True)[0]# (30, 1)\n",
    "    d2udt2 = torch.autograd.grad(dudt, t_physics, torch.ones_like(dudt), create_graph=True)[0]# (30, 1)\n",
    "    loss3 = torch.mean((d2udt2 + mu*dudt + k*u)**2)\n",
    "    \n",
    "    # backpropagate joint loss, take optimiser step\n",
    "    loss = loss1 + lambda1*loss2 + lambda2*loss3\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if i % 5000 == 0: \n",
    "        #print(u.abs().mean().item(), dudt.abs().mean().item(), d2udt2.abs().mean().item())\n",
    "        u = pinn(t_test).detach()\n",
    "        plt.figure(figsize=(6,2.5))\n",
    "        plt.scatter(t_physics.detach()[:,0], \n",
    "                    torch.zeros_like(t_physics)[:,0], s=20, lw=0, color=\"tab:green\", alpha=0.6)\n",
    "        plt.scatter(t_boundary.detach()[:,0], \n",
    "                    torch.zeros_like(t_boundary)[:,0], s=20, lw=0, color=\"tab:red\", alpha=0.6)\n",
    "        plt.plot(t_test[:,0], u_exact[:,0], label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n",
    "        plt.plot(t_test[:,0], u[:,0], label=\"PINN solution\", color=\"tab:green\")\n",
    "        plt.title(f\"Training step {i}\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
