{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Maziar Raissi\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../Utilities/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X, u, layers, lb, ub):\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "        self.x = X[:,0:1]\n",
    "        self.t = X[:,1:2]\n",
    "        self.u = u\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # tf placeholders and graph\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                                     log_device_placement=True))\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
    "        self.lambda_2 = tf.Variable([-6.0], dtype=tf.float32)\n",
    "        \n",
    "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "        self.t_tf = tf.placeholder(tf.float32, shape=[None, self.t.shape[1]])\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "                \n",
    "        self.u_pred = self.net_u(self.x_tf, self.t_tf)\n",
    "        self.f_pred = self.net_f(self.x_tf, self.t_tf)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.f_pred))\n",
    "        \n",
    "        self.optimizer = tf.contrib.opt.ScipyOptimizerInterface(self.loss, \n",
    "                                                                method = 'L-BFGS-B', \n",
    "                                                                options = {'maxiter': 50000,\n",
    "                                                                           'maxfun': 50000,\n",
    "                                                                           'maxcor': 50,\n",
    "                                                                           'maxls': 50,\n",
    "                                                                           'ftol' : 1.0 * np.finfo(float).eps})\n",
    "    \n",
    "        self.optimizer_Adam = tf.train.AdamOptimizer()\n",
    "        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def initialize_NN(self, layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0,num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
    "        for l in range(0,num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "            \n",
    "    def net_u(self, x, t):  \n",
    "        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        lambda_1 = self.lambda_1        \n",
    "        lambda_2 = tf.exp(self.lambda_2)\n",
    "        u = self.net_u(x,t)\n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        f = u_t + lambda_1*u*u_x - lambda_2*u_xx\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def callback(self, loss, lambda_1, lambda_2):\n",
    "        print('Loss: %e, l1: %.5f, l2: %.5f' % (loss, lambda_1, np.exp(lambda_2)))\n",
    "        \n",
    "        \n",
    "    def train(self, nIter):\n",
    "        tf_dict = {self.x_tf: self.x, self.t_tf: self.t, self.u_tf: self.u}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            self.sess.run(self.train_op_Adam, tf_dict)\n",
    "            \n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                loss_value = self.sess.run(self.loss, tf_dict)\n",
    "                lambda_1_value = self.sess.run(self.lambda_1)\n",
    "                lambda_2_value = np.exp(self.sess.run(self.lambda_2))\n",
    "                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f, Time: %.2f' % \n",
    "                      (it, loss_value, lambda_1_value, lambda_2_value, elapsed))\n",
    "                start_time = time.time()\n",
    "        \n",
    "        self.optimizer.minimize(self.sess,\n",
    "                                feed_dict = tf_dict,\n",
    "                                fetches = [self.loss, self.lambda_1, self.lambda_2],\n",
    "                                loss_callback = self.callback)\n",
    "        \n",
    "        \n",
    "    def predict(self, X_star):\n",
    "        \n",
    "        tf_dict = {self.x_tf: X_star[:,0:1], self.t_tf: X_star[:,1:2]}\n",
    "        \n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        f_star = self.sess.run(self.f_pred, tf_dict)\n",
    "        \n",
    "        return u_star, f_star\n",
    "\n",
    "#plotのためのutility関数\n",
    "def figsize(scale, nplots = 1):\n",
    "    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
    "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
    "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
    "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
    "    fig_height = nplots*fig_width*golden_mean              # height in inches\n",
    "    fig_size = [fig_width,fig_height]\n",
    "    return fig_size\n",
    "\n",
    "def newfig(width, nplots = 1):\n",
    "    fig = plt.figure(figsize=figsize(width, nplots))\n",
    "    ax = fig.add_subplot(111)\n",
    "    return fig, ax\n",
    "\n",
    "def savefig(filename, crop = True):\n",
    "    if crop == True:\n",
    "#        plt.savefig('{}.pgf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
    "        plt.savefig('{}.pdf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
    "        plt.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\n",
    "    else:\n",
    "#        plt.savefig('{}.pgf'.format(filename))\n",
    "        plt.savefig('{}.pdf'.format(filename))\n",
    "        plt.savefig('{}.eps'.format(filename))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "     \n",
    "    nu = 0.01/np.pi\n",
    "\n",
    "    N_u = 2000\n",
    "    layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "    \n",
    "    data = scipy.io.loadmat('../Data/burgers_shock.mat')\n",
    "    \n",
    "    t = data['t'].flatten()[:,None]\n",
    "    x = data['x'].flatten()[:,None]\n",
    "    Exact = np.real(data['usol']).T\n",
    "    \n",
    "    X, T = np.meshgrid(x,t)\n",
    "    \n",
    "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "    u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "    # Doman bounds\n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)    \n",
    "    \n",
    "    ######################################################################\n",
    "    ######################## Noiseles Data ###############################\n",
    "    ######################################################################\n",
    "    noise = 0.0            \n",
    "             \n",
    "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "    X_u_train = X_star[idx,:]\n",
    "    u_train = u_star[idx,:]\n",
    "    \n",
    "    model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "    model.train(0)\n",
    "    \n",
    "    u_pred, f_pred = model.predict(X_star)\n",
    "            \n",
    "    error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "    \n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "        \n",
    "    lambda_1_value = model.sess.run(model.lambda_1)\n",
    "    lambda_2_value = model.sess.run(model.lambda_2)\n",
    "    lambda_2_value = np.exp(lambda_2_value)\n",
    "    \n",
    "    error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
    "    error_lambda_2 = np.abs(lambda_2_value - nu)/nu * 100\n",
    "    \n",
    "    print('Error u: %e' % (error_u))    \n",
    "    print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "    print('Error l2: %.5f%%' % (error_lambda_2))  \n",
    "    \n",
    "    ######################################################################\n",
    "    ########################### Noisy Data ###############################\n",
    "    ######################################################################\n",
    "    noise = 0.01        \n",
    "    u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
    "        \n",
    "    model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "    model.train(10000)\n",
    "    \n",
    "    u_pred, f_pred = model.predict(X_star)\n",
    "        \n",
    "    lambda_1_value_noisy = model.sess.run(model.lambda_1)\n",
    "    lambda_2_value_noisy = model.sess.run(model.lambda_2)\n",
    "    lambda_2_value_noisy = np.exp(lambda_2_value_noisy)\n",
    "            \n",
    "    error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0)*100\n",
    "    error_lambda_2_noisy = np.abs(lambda_2_value_noisy - nu)/nu * 100\n",
    "    \n",
    "    print('Error lambda_1: %f%%' % (error_lambda_1_noisy))\n",
    "    print('Error lambda_2: %f%%' % (error_lambda_2_noisy))                           \n",
    "\n",
    " \n",
    "    ######################################################################\n",
    "    ############################# Plotting ###############################\n",
    "    ######################################################################    \n",
    "    \n",
    "    fig, ax = newfig(1.0, 1.4)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ####### Row 0: u(t,x) ##################    \n",
    "    gs0 = gridspec.GridSpec(1, 2)\n",
    "    gs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\n",
    "    ax = plt.subplot(gs0[:, :])\n",
    "    \n",
    "    h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "                  extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                  origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "    \n",
    "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 2, clip_on = False)\n",
    "    \n",
    "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    \n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\n",
    "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "    \n",
    "    ####### Row 1: u(t,x) slices ##################    \n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')    \n",
    "    ax.set_title('$t = 0.25$', fontsize = 10)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 1])\n",
    "    ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "    ax.set_title('$t = 0.50$', fontsize = 10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 2])\n",
    "    ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])    \n",
    "    ax.set_title('$t = 0.75$', fontsize = 10)\n",
    "    \n",
    "    ####### Row 3: Identified PDE ##################    \n",
    "    gs2 = gridspec.GridSpec(1, 3)\n",
    "    gs2.update(top=1.0-2.0/3.0, bottom=0, left=0.0, right=1.0, wspace=0.0)\n",
    "    \n",
    "    ax = plt.subplot(gs2[:, :])\n",
    "    ax.axis('off')\n",
    "    s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.0031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n",
    "    s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
    "    s3 = r'Identified PDE (1\\% noise) & '\n",
    "    s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
    "    s5 = r'\\end{tabular}$'\n",
    "    s = s1+s2+s3+s4+s5\n",
    "    ax.text(0.1,0.1,s)\n",
    "        \n",
    "    # savefig('./figures/Burgers_identification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PhysicsInformedNN' object has no attribute 'sess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 178\u001b[0m\n\u001b[1;32m    175\u001b[0m model \u001b[38;5;241m=\u001b[39m PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n\u001b[1;32m    176\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m u_pred, f_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_star\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m error_u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(u_star \u001b[38;5;241m-\u001b[39m u_pred, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(u_star, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    182\u001b[0m U_pred \u001b[38;5;241m=\u001b[39m griddata(X_star, u_pred\u001b[38;5;241m.\u001b[39mflatten(), (X, T), method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcubic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 138\u001b[0m, in \u001b[0;36mPhysicsInformedNN.predict\u001b[0;34m(self, X_star)\u001b[0m\n\u001b[1;32m    135\u001b[0m x_tf_star \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(X_star[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    136\u001b[0m t_tf_star \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(X_star[:,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 138\u001b[0m u_star \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msess\u001b[49m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu_pred, {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_tf: x_tf_star, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_tf: t_tf_star})\n\u001b[1;32m    139\u001b[0m f_star \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_pred, {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_tf: x_tf_star, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_tf: t_tf_star})\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u_star, f_star\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PhysicsInformedNN' object has no attribute 'sess'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: Maziar Raissi\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "# from plotting import newfig, savefig\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "import time\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "class PhysicsInformedNN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X, u, layers, lb, ub):\n",
    "        \n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        \n",
    "        self.x = X[:,0:1]\n",
    "        self.t = X[:,1:2]\n",
    "        self.u = u\n",
    "        \n",
    "        self.layers = layers\n",
    "        \n",
    "        # Initialize NNs\n",
    "        self.weights, self.biases = self.initialize_NN(layers)\n",
    "        \n",
    "        # tf placeholders and graph\n",
    "        self.lambda_1 = tf.Variable([0.0], dtype=tf.float32)\n",
    "        self.lambda_2 = tf.Variable([-6.0], dtype=tf.float32)\n",
    "        \n",
    "        self.x_tf = tf.convert_to_tensor(self.x, dtype=tf.float32)\n",
    "        self.t_tf = tf.convert_to_tensor(self.t, dtype=tf.float32)\n",
    "        self.u_tf = tf.convert_to_tensor(self.u, dtype=tf.float32)\n",
    "                \n",
    "        self.u_pred = self.net_u(self.x_tf, self.t_tf)\n",
    "        self.f_pred = self.net_f(self.x_tf, self.t_tf)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.u_tf - self.u_pred)) + \\\n",
    "                    tf.reduce_mean(tf.square(self.f_pred))\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "    def initialize_NN(self, layers):        \n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(layers) \n",
    "        for l in range(0, num_layers-1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1, layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)        \n",
    "        return weights, biases\n",
    "        \n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]        \n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "    \n",
    "    def neural_net(self, X, weights, biases):\n",
    "        num_layers = len(weights) + 1\n",
    "        \n",
    "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
    "        for l in range(0, num_layers-2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "            \n",
    "    def net_u(self, x, t):  \n",
    "        u = self.neural_net(tf.concat([x, t], 1), self.weights, self.biases)\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        lambda_1 = self.lambda_1        \n",
    "        lambda_2 = tf.exp(self.lambda_2)\n",
    "        # u = self.net_u(x, t)\n",
    "        # u_t = tf.gradients(u, t)[0]\n",
    "        # u_x = tf.gradients(u, x)[0]\n",
    "        # u_xx = tf.gradients(u_x, x)[0]\n",
    "        # f = u_t + lambda_1 * u * u_x - lambda_2 * u_xx\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(t)\n",
    "            tape.watch(x)\n",
    "            \n",
    "            u = self.net_u(x, t)\n",
    "            u_t = tape.gradient(u, t)\n",
    "            u_x = tape.gradient(u, x)\n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "        del tape  # 不要なGradientTapeを解放\n",
    "        f = u_t + lambda_1 * u * u_x - lambda_2 * u_xx\n",
    "        return f\n",
    "    \n",
    "    def callback(self, loss, lambda_1, lambda_2):\n",
    "        print('Loss: %e, l1: %.5f, l2: %.5f' % (loss, lambda_1, np.exp(lambda_2)))\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, tf_dict):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss\n",
    "\n",
    "        gradients = tape.gradient(loss, [self.lambda_1, self.lambda_2] + self.weights + self.biases)\n",
    "        self.optimizer.apply_gradients(zip(gradients, [self.lambda_1, self.lambda_2] + self.weights + self.biases))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, nIter):\n",
    "        # tf_dict = {self.x_tf: self.x, self.t_tf: self.t, self.u_tf: self.u}\n",
    "        tf_dict = {self.x_tf.ref(): self.x, self.t_tf.ref(): self.t, self.u_tf.ref(): self.u}\n",
    "        start_time = time.time()\n",
    "        for it in range(nIter):\n",
    "            loss_value = self.train_step(tf_dict)\n",
    "            \n",
    "            # Print\n",
    "            if it % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                lambda_1_value = self.lambda_1.numpy()\n",
    "                lambda_2_value = np.exp(self.lambda_2.numpy())\n",
    "                print('It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f, Time: %.2f' % \n",
    "                      (it, loss_value, lambda_1_value, lambda_2_value, elapsed))\n",
    "                start_time = time.time()\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        \n",
    "        x_tf_star = tf.convert_to_tensor(X_star[:,0:1], dtype=tf.float32)\n",
    "        t_tf_star = tf.convert_to_tensor(X_star[:,1:2], dtype=tf.float32)\n",
    "        \n",
    "        u_star = self.sess.run(self.u_pred, {self.x_tf: x_tf_star, self.t_tf: t_tf_star})\n",
    "        f_star = self.sess.run(self.f_pred, {self.x_tf: x_tf_star, self.t_tf: t_tf_star})\n",
    "        \n",
    "        return u_star, f_star\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "     \n",
    "    nu = 0.01 / np.pi\n",
    "\n",
    "    N_u = 2000\n",
    "    layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "    \n",
    "    data = scipy.io.loadmat('./Data/burgers_shock.mat')\n",
    "    \n",
    "    t = data['t'].flatten()[:, None]\n",
    "    x = data['x'].flatten()[:, None]\n",
    "    Exact = np.real(data['usol']).T\n",
    "    \n",
    "    X, T = np.meshgrid(x, t)\n",
    "    \n",
    "    X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "    u_star = Exact.flatten()[:, None]              \n",
    "\n",
    "    # Domain bounds\n",
    "    lb = X_star.min(0)\n",
    "    ub = X_star.max(0)    \n",
    "    \n",
    "    ######################################################################\n",
    "    ######################## Noiseless Data ##############################\n",
    "    ######################################################################\n",
    "    noise = 0.0            \n",
    "             \n",
    "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "    X_u_train = X_star[idx, :]\n",
    "    u_train = u_star[idx, :]\n",
    "    \n",
    "    model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "    model.train(0)\n",
    "    \n",
    "    u_pred, f_pred = model.predict(X_star)\n",
    "            \n",
    "    error_u = np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
    "    \n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "        \n",
    "    lambda_1_value = model.lambda_1.numpy()\n",
    "    lambda_2_value = np.exp(model.lambda_2.numpy())\n",
    "    \n",
    "    error_lambda_1 = np.abs(lambda_1_value - 1.0) * 100\n",
    "    error_lambda_2 = np.abs(lambda_2_value - nu) / nu * 100\n",
    "    \n",
    "    print('Error u: %e' % (error_u))    \n",
    "    print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "    print('Error l2: %.5f%%' % (error_lambda_2))  \n",
    "    \n",
    "    ######################################################################\n",
    "    ########################### Noisy Data ###############################\n",
    "    ######################################################################\n",
    "    noise = 0.01        \n",
    "    u_train = u_train + noise * np.std(u_train) * np.random.randn(u_train.shape[0], u_train.shape[1])\n",
    "        \n",
    "    model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "    model.train(10000)\n",
    "    \n",
    "    u_pred, f_pred = model.predict(X_star)\n",
    "        \n",
    "    lambda_1_value_noisy = model.lambda_1.numpy()\n",
    "    lambda_2_value_noisy = np.exp(model.lambda_2.numpy())\n",
    "            \n",
    "    error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0) * 100\n",
    "    error_lambda_2_noisy = np.abs(lambda_2_value_noisy - nu) / nu * 100\n",
    "    \n",
    "    print('Error lambda_1: %f%%' % (error_lambda_1_noisy))\n",
    "    print('Error lambda_2: %f%%' % (error_lambda_2_noisy))                           \n",
    "\n",
    " \n",
    "    ######################################################################\n",
    "    ############################# Plotting ###############################\n",
    "    ######################################################################    \n",
    "    \n",
    "    # fig, ax = newfig(\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
